{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Root Finding and Optimization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Root finding and optimization are core numerical techniques that enable us to solve complex equations and optimize functions in fields where analytical solutions are often impossible.\n",
    "Root finding aims to determine values for which a function $f(x) = 0$, and finds application across engineering, physics, and financeâ€”whether calculating stresses in materials, energy levels in quantum mechanics, or rates of return in investments.\n",
    "Optimization seeks to find the minimum or maximum of a function and is especially crucial in machine learning, where minimizing loss functions directly affects model performance.\n",
    "The two concepts intersect in gradient-based optimization, where finding the roots of a gradient helps locate stationary points and optimize complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Root Finding Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Bisection Method\n",
    "\n",
    "The Bisection Method is a simple and robust root-finding algorithm that relies on the Intermediate Value Theorem.\n",
    "The theorem states that if $f(x)$ is a continuous function on an interval $[a, b]$ and $f(a)$ and $f(b)$ have opposite signs, then there exists at least one root in the interval $(a, b)$ where $f(x) = 0$.\n",
    "We already implemented a similar algorithm in a [previous lecture](../06/interpolate.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisection_index(xs, target):\n",
    "    l, h = 0, len(xs) - 1\n",
    "    while h - l > 1:\n",
    "        m = (h + l) // 2\n",
    "        if target >= xs[m]:\n",
    "            l = m\n",
    "        else:\n",
    "            h = m\n",
    "    return l # returns index of the closest value less than or equal to target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "The main difference is that we no longer have a finite set of sampling points.\n",
    "Instead, low `l` and high `h` bounds are given with a error tolerance `tol`.\n",
    "Also, the target is alway zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisection(f, l, h, tol=1e-6):\n",
    "    if f(l) * f(h) >= 0:\n",
    "        raise ValueError(\"f(a) and f(b) must have opposite signs\")\n",
    "    while h - l > 2*tol:\n",
    "        m = (l + h) / 2\n",
    "        if f(m) == 0:\n",
    "            return m  # m is the root by definition\n",
    "        elif f(l) * f(m) > 0:\n",
    "            l = m\n",
    "        else:\n",
    "            h = m\n",
    "    return (l + m) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**3 - x - 2\n",
    "\n",
    "root = bisection(f, 1, 2)\n",
    "print(\"Approximate root:\")\n",
    "print(\"  x0  = \",   root )\n",
    "print(\"f(x0) = \", f(root))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "The Bisection Method is guaranteed to converge to a root if the function is continuous on $[l, h]$ and $f(l)$ and $f(h)$ have different signs.\n",
    "However, its convergence rate is relatively slow, decreasing the interval size by half with each iteration.\n",
    "This results in a linear convergence rate (of error at fixed step)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Newton-Raphson Method\n",
    "\n",
    "The Newton-Raphson Method is based on the concept of using the tangent line at a point on the curve of a function to approximate its root.\n",
    "It leverages the Taylor series expansion to iteratively move closer to the root, achieving quadratic convergence when the initial guess is close to the actual root."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Consider a function $f(x)$ that we want to find the root.\n",
    "Suppose we have an initial guess $x_0$ near the root.\n",
    "We can expand $f(x)$ around this point $x_0$ using the Taylor series:\n",
    "\\begin{align}\n",
    "f(x) = f(x_0) + f'(x_0)(x - x_0) + \\frac{f''(x_0)}{2}(x - x_0)^2 + \\cdots\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "For simplicity, we approximate the function linearly by ignoring the higher-order terms.\n",
    "This gives us a linear approximation:\n",
    "\\begin{align}\n",
    "f(x) \\approx f(x_0) + f'(x_0)(x - x_0)\n",
    "\\end{align}\n",
    "\n",
    "We want to find the value of $x$ where $f(x) = 0$.\n",
    "Therefore,\n",
    "\\begin{align}\n",
    "x \\approx x_0 - \\frac{f(x_0)}{f'(x_0)}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "We may call this next approximation $x_{n+1}$.\n",
    "Thus, we define the iterative update as:\n",
    "\\begin{align}\n",
    "x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\n",
    "\\end{align}\n",
    "\n",
    "This formula is the foundation of the Newton-Raphson Method.\n",
    "It has a simple geometric interpretation.\n",
    "Starting with an initial guess $x_0$, we compute the tangent line to the function $f(x)$ at $x_0$.\n",
    "The tangent line provides a linear approximation of $f(x)$ near $x_0$, and where this tangent crosses the $x$-axis is our next estimate for the root, $x_1$.\n",
    "By iterating this process, we continue to update our estimate, ideally converging quickly to the root."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "The Newton-Raphson Method is quadratically convergence near the root, which means that the error approximately squares with each iteration.\n",
    "Specifically, if $x_n$ is close enough to the root, the error at the next step, $|x_{n+1} - x_\\infty|$, is roughly proportional to $|x_n - x_\\infty|^2$.\n",
    "This quadratic convergence makes the Newton-Raphson Method highly efficient when close to the root.\n",
    "However, it requires:\n",
    "* **Non-Zero Derivative:** The method requires that $f'(x) \\neq 0$ at each iteration.\n",
    "  If $f'(x) = 0$ at any point, the update formula becomes undefined, and the algorithm will fail.\n",
    "* **Good Initial Guess:** Convergence to the root is not guaranteed if the initial guess $x_0$ is too far from the actual root.\n",
    "  Poor starting points can lead the method to diverge or to converge to the wrong root.\n",
    "* **Well-Behaved Function:** The method performs best when $f(x)$ is smooth and continuous near the root."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Here is a Python function that implements the Newton-Raphson Method with an initial guess and a tolerance level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(f, df, x0, tol=1e-6, imax=100):\n",
    "    for _ in range(imax):\n",
    "        f0, df0 = f(x0), df(x0)\n",
    "        if df0 == 0:\n",
    "            raise ValueError(\"Derivative is zero. No convergence.\")\n",
    "        x = x0 - f0 / df0\n",
    "        if abs(x - x0) < tol:\n",
    "            return x\n",
    "        x0 = x\n",
    "    raise ValueError(\"Maximum iterations reached without convergence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "f  = lambda x: x**3 - x - 2\n",
    "df = lambda x: 3*x**2 - 1\n",
    "\n",
    "initial_guess = 1.5\n",
    "root = newton(f, df, initial_guess)\n",
    "print(\"Approximate root:\")\n",
    "print(\"  x0  = \",   root )\n",
    "print(\"f(x0) = \", f(root))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "The Newton-Raphson Method is fast and efficient, especially when close to the root, due to its quadratic convergence.\n",
    "However, in addition to the convergence conditions list above, it requires computing derivatives, making it less convenient compared to the bisection method.\n",
    "\n",
    "Nevertheless, we learned about different derivative methods in this course, specifically, the machine accurate autodiff method.\n",
    "By using it, we can implement a convenient Newton-Raphson Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "from jax import grad\n",
    "\n",
    "def autonewton(f, x0, tol=1e-6, imax=100):\n",
    "    df = grad(f)\n",
    "    for _ in range(imax):\n",
    "        f0, df0 = f(x0), df(x0)\n",
    "        if df0 == 0:\n",
    "            raise ValueError(\"Derivative is zero. No convergence.\")\n",
    "        x = x0 - f0 / df0\n",
    "        if abs(x - x0) < tol:\n",
    "            return x\n",
    "        x0 = x\n",
    "    raise ValueError(\"Maximum iterations reached without convergence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guess = 1.5\n",
    "root = autonewton(f, initial_guess)\n",
    "print(\"Approximate root:\")\n",
    "print(\"  x0  = \",   root )\n",
    "print(\"f(x0) = \", f(root))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
